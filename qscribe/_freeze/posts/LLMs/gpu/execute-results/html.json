{
  "hash": "1a57fd347516b8abb4d972b2a026ba41",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to GPU\"\nauthor: \"Rakesh Venkat\"\ndate: \"2025-03-01\"\ncategories: [LLM]\nimage: \"img/CUDAWorkflow.png\"\njupyter: python3\n---\n\n# Introduction\n\nIn my last blog, I quickly introduced to DeepSeek and some of the components the used. In this blog, we will get down to basics of GPU and specifically NVIDIA GPU architecture, how CUDA programs gets compiled.\n\n------------------------------------------------------------------------\n\n## *CPU vs GPU*\n\nWhat is the difference between a CPU and GPU?\n\n**A**\\> CPUs are for general-purpose computing where as GPUs are optimized for performing same operations on multuple data points simultaneously achieveing in high level of parallelism.\n\n**B**\\> CPUs are best suited which runnign complex logic where as GPUs are ideally for massively parallel computations like graphics rendering, deep learning and scientific simulations.\n\n**C**\\> CPU is optimized for low-latency access to relatively small amount of memory. GPU is typically used for high-bandwidth to large amount of dataset in parallel.\n\n## *GPU*\n\nHere's a quick introduction to GPU, it's architecture and CUDA programming workflow.\n\nThe cuda program workflow is as shown below. It starts off with your program which has extension of \".cu\". This code can be written in C, C++ or Fortran.\n\n## *Basic Flow*\n\nThis is a very basic flow of a CUDA program.\n\n![Cuda Program Overview](img/CUDAOverview.png)\n\n## *Detailed Workflow*\n\n![CUDA Workflow](img/CUDAWorkflow.png)\n\nTypically these programs has both CPU and GPU instructions running on host machine. The CPU piece of code is called the host code and GPU code section is called device code typically which has **global** and **device** function.\n\nThis code is compiled using NVCC compiler separating both the CPU and GPU code. The CPU code uses CPU complier like `GCC` converting to the object code.\n\nAt the same time the 1st pass of the GPU converts the GPU code is converted to PTX code. This is a low level IR (Intermediate Representation) is then compiled to convert it to device specific code known as SASS code.\n\nFinal stage is to link the host object code with SASS and run the code to run on host machine.\n\nPTX is an abstraction layer helping code portabiloty between NVIDIA decides. This helps tools and libraries to manupulate code code before GPU execution, an approach whichDeepSeek team did. We will have a separate article going over the PTX.\n\n## *Structure*\n\nBefore we delve deeper into the PTX optimization by DeepSeek, let us first talk about GPU .\n\n![GPU Structure](img/GPU%20Structure.png)\n\nThere are : 1. 7 Graphic Processing Clusters (GPC) 2. Each GPC had 12 Streaming Multiprocessors (SM). 3. Every SM will have 4 Warps and 1 Ray Tracing core 4. A single Warp will have 32 CUDA core and 1 Tensor Code. 5. 12 Graphic Memory Controllers 6. Two L2 Cache of 6MB SRAM each 7. NVLink 8. PCIe Interface\n\nSo in total in one NVDIA GPU there could be about :\n\n> `10752` CUDA cores\n\n> `336` Tensor cores\n\n> `84` Ray tracing cores\n\n## *Core*\n\n![CUDA Cores](img/Cores.png)\n\nHere are the three cores available in these GPUs\n\n-   CUDA Core is used for game and game engines.\n-   Tensor Core is exclusively for Matrix Multiplication and Geometric Transformation which is used in AI/ML\n-   Ray Tracing is used ror Ray Tracing algorithms.\n\n## Next Steps\n\nWith basic introduction to GPU out of the way. let us move to PTX. See you in my next arcticle.\n\n",
    "supporting": [
      "gpu_files"
    ],
    "filters": [],
    "includes": {}
  }
}