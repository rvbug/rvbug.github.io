{
  "hash": "592033f4f2059c7dfa5af89fc91d9d24",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenizer\"\nauthor: \"Rakesh Venkat\"\ndate: \"2025-03-01\"\ncategories: [LLM]\nimage: \"img/EmbeddingMatrix.png\"\njupyter: python3\n---\n\n------------------------------------------------------------------------\n\n# Introduction\n\nIn building LLMs, the first block is called input block. In this step, input text passed through the Tokenizer to create a tokenized text. This Token IDs are then passed through Embedding Layer and positional encoding is added before sending it to the transformer block.\n\n<!-- truncate -->\n\nI will talk about positional encoding later in the series. For now think of ids (`numerical values`) been sent to transformer block.\n\nHere's the architecture of LLMs showing all three blocks.\n\n![LLM Achitecture](img/LLMarch.png)\n\n# Tokenizer\n\nInput to the transformers are numbers not words, so we need to change the raw text to tokens (usually words, subwords or even characters) and convert it to token ids. This is called Tokenization.\n\nThe token ids are then passed to a neural network which transforms to continuous vector representations in a very high dimensional space. These dense vector representations capture semantic relationships between tokens, allowing machines to process language more effectively.\n\nHere's how Embedding Matrix are typically created.\\\nIf you follow the text **`This`** - If token id is 2 , the embedding matrix could be **`[0.2, 0.8....]`**\n\n![Embedding Matrix](img/EmbeddingMatrix.png)\n\n::: info\n`Embedding Matrix` - A lookup table where each row corresponds to a specific token in the vocabulary and contains that token's learned vector representation.\n:::\n\nHere's the summary -\n\n![Tokenization](img/tokenization.png)\n\n::: info\nTypically these Embedding size (rows) are between 768 to roughly 16K for each column inputs text.\n:::\n\n## Types of Tokenizer\n\nTo tokenize the input text, there are three types which are available. `Sub-Word` based tokenizers are what is typically used in LLMs.\\\n![alt text](img/tokenizer.png)\n\nByte Pair Encoding (BPE) is one such library.\n\n## Byte Pair Encoding (BPE)\n\n::: info\n-   Orginally BPE was developed as a compression algorithm.\n\n-   Checkout more details and the example used from [**Wiki**](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n:::\n\n![alt text](img/Bpe.png)\n\n## Modified BPE\n\n::: tip\nIn LLM, we use something called **`Modified Byte Pair Encoding`**. Used for encoding plain text to tokens\n:::\n\n![alt text](img/ModifiedBPE.png)\n\n## BPE Library\n\nIn Rust, you can use [Tiktoken-rs](https://docs.rs/crate/tiktoken-rs/latest) for *`BPE`*\n\n::: info\nEquivalent Python library is available\n\n> [Tiktoken](https://pypi.org/project/tiktoken/)\n:::\n\n## Rust Code\n\nBelow is the rust code using `Tiktoken-rs`\n\n``` rust\nuse tiktoken_rs::o200k_base;\n\nfn main() {\n    let bpe = o200k_base().unwrap();\n    let token: Vec<u32> = bpe.encode_with_special_tokens(\"This is multi line sentence for BPE with rust and a sentence   with spaces\");\n\n    println!(\"Token: {:?}\", token);\n    println!(\"Decoding the token {:?}\", bpe.decode(token));\n\n}\n```\n\nOutput of this program is as shown below\n\n``` bash\n$> cargo run\n\n## OUTPUT\n\n#Token: [2500, 382, 12151, 2543, 21872, 395, 418, 3111, 483, 20294, 326, 261, 21872, 256, 483, 18608]\n#Decoding the token Ok(\"This is multi line sentence for BPE with rust and a sentence   with spaces\")\n```\n\n## Python Code\n\n``` python\nimport tiktoken\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\n\ntext_data = ( \"Ecode using BPE via Python\")\n\nencoder_output = tokenizer.encode(text_data, allowed_special={\"<|END|>\"})\nprint(encoder_output)\n\n\ndecoder_output = tokenizer.decode(encoder_output)\n\nprint(decoder_output)\n```\n\n``` bash\n$> python3 bpe_example.py\n\n## OUTPUT\n\n# [36, 3056, 2360, 418, 3111, 4493, 26534]\n# Ecode using BPE via Python\n```\n\n",
    "supporting": [
      "Tokenizer_files"
    ],
    "filters": [],
    "includes": {}
  }
}