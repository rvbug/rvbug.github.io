{
  "hash": "4e3e385967fb409d73155bf537c13543",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"DeepSeekPTX\"\nauthor: \"Rakesh Venkat\"\ndate: \"2025-03-01\"\ncategories: [LLM]\nimage: \"img/deepseek.jpg\"\njupyter: python3\n---\n\n# Introduction\n\nLet us now delve into the details of PTX, the parallel thread execution virtual instruction set, and explore how DeepSeek might have approached optimization for their H800 GPUs. PTX optimization is critical for maximizing performance in large language models.\n\n<!-- truncate -->\n\nSince DeepSeek's specific PTX implementations are proprietary, this article focuses on optimization strategies inferred from their research papers and related discussions. We'll explore a few of them within their architecture. For example, **Multi-Head Latent Attention (MHLA)** employs a modified Key and Value cache approach, differing from the standard transformer KV cache concept, to enhance efficiency.\n\n------------------------------------------------------------------------\n\n# Overview\n\nDeepSeek, particularly with their R1 model, has implemented significant optimizations across both training and inference phases. We will delve into these broader optimizations in a separate, more detailed article. In this one, however, our focus will be exclusively on PTX. Â \n\nPTX empowers developers with the ability to perform low-level optimizations, granting fine-grained control over register allocation, thread execution, and memory access patterns.\n\n## Register Allocation\n\n1.  Allowing manually optimizing the register allocation, the latency could have been reduced.\n\n2.  Fine-tune thread scheduling allowing them to maximize parallelism across the streaming multiprocessors.\n\n## Custom Memory Management\n\nImplementing custome PTX instructions for accessing memory including global VRAM access by bypassing L1 and L2 cache in a very specific way allowing increasing data transfer pattern thus improving memory bandwidth.\n\n::: tip\n-   **Global VRAM** is largest and slowest memory on the GPU\n-   **Cache** can also introduce overhead and may not always be effective\n-   **Coalesced Access** - Accessing contiguous memory locations in a single transaction significantly improves memory bandwidth.\n-   **Memory Access** - Aligned memory access e.g. to 128-bytes are much more efficient.\n:::\n\n### Cache\n\nSince they were dealing with large and streaming datasets, they miht have bypassed **`L1`** or **`L2`** cache. This can be accessible via PTX that allow to control these behaviour\n\nBelow is the sample snippet showing the access. Loads from global memory and bypass both L1 and L2 cache.\n\n``` asm\n.reg .u64 %addr;\n.reg .f32 %data;\n\nld.global.nc.f32 %data, [%addr]; \n```\n\n::: info\n> **nc** means no cache\\\n> **ld.global.nc.f32** - load 32 bit floating point value from global memory\n:::\n\n### Cache Controls\n\n`.volalite` - This modifer tells compiler that memory location can be modified by other threads/devices preventing complier for any optimization to ensure value in the memory remains constant.\n\n`.wt` and `.wb` - These are write through and write back modifiers controling the cache write policy.\n\n`.wt` writes to both cache and global memory while `.wb` writes only to cache but writes to global memory once cache data is evicted.\n\n::: info\nDeepseek might have used these `write-through` and `write-back` modifiers to further optimize their workload.\n:::\n\n`.relaxed`,`.acquire`,`.release`,`.acquire_release` modifiers are used when dealing with memory coherency between threads i.e. order of memory reads and writes\n\n::: info\nDeepseek most likely used these modifiers when working with shared memory buffers which are accessed by multiple threads.\n:::\n\n### Prefetching\n\nFor the predictible memory access, they could have use PTX's prefetch instructions to bring load the data in cache before it is needed hiding memory latency thus improving performance\n\n``` ptx\nreg .u64 %addr;\nprefetch.global [%addr];\n```\n\n::: info\n> **prefetch.global** Prefetch data into L1 cache.\n:::\n\n#### Prefetch Distance and Hints\n\nIt is possible that these parameters are tuned to optimizing prefetching performance.\n\n::: info\n> **Prefetch Distance** Number of memory location to prefetch ahead.\n\n> **Prefetch Hints** helps to understand tyoe of memory access patterns based on the type of hardware.\n:::\n\n### Alignment & Coalescing\n\nSince PTX allow precise control over memory aligment and access patterns, they could use this to maximize memory bandwidth. Sample code below.\n\n``` ptx\n.reg .u64 %base_addr;\n.reg .u32 %offset;\n.reg .f32 %data;\n\nmad.lo.u64 %addr, %offset, 4, %base_addr; // Assuming 4-byte floats\n\n// Load coalesced data\nld.global.v4.f32 {%data, %data+4, %data+8, %data+12}, [%addr];\n```\n\n::: info\n> **ld.global.v4.f32** - Loads vector of 4 32-bit floating values from VRAM ensuring coalesced access.\n\n> **mad.lo.u64** - Multiply add lower 64 bits for calculating memory address.\n:::\n\n#### Vectorized loads\n\nThey might have used vectorized loads which allow multiple data element to be transferred into a single memory transactions by maximizing memory bandwidth and also ensure these access are coalesced. Sample code below showing loading and storing 4 floats at once.\n\n``` ptx\n.reg .u64 %addr;\n.reg .v4.f32 %data;\n\nld.global.v4.f32 %data, [%addr]; \nst.global.v4.f32 [%addr], %data;\n```\n\n### Shared Memory Optimization\n\nShared memory is organized in bands so to avoid conflicts they could have used multiple threads access the same banks simultaneously by arranging data carefully.\n\nIt could also be possible that they might have used shared on-chip memory to reduce global access. Below code shows data being moved from global memory to shared memory and then use it.\n\n``` ptx\n.shared .f32 shared_data[1024];\n.reg .u32 %thread_id;\n.reg .f32 %local_data;\n\n// Load data from global memory into shared memory\nld.global.f32 %local_data, [global_addr + %thread_id*4];\nst.shared.f32 [shared_data + %thread_id*4], %local_data;\n\n// Use data from shared memory\nld.shared.f32 %local_data, [shared_data + %thread_id*4];\n```\n\n## Inter-GPU communcation\n\nAllocate a portion of SM to improve communication by data compression and remove bottlenecks\n\n## Warp Level Optimization\n\nFine-grain tunining again on warp which contains 32 threads on how they process instructions.\n\nNVIDIA GPUs execute threads in groups of 32, called warps. So PTX can allow developers to write warp-synchronous code, to make it more efficient.\n\nDeepSeek could have used warp-level primitives to perform warp-wide reductions and scans.\n\n#### Warp Shuffle Instructions:\n\nPTX also provides shuffle instructions that allow threads within a warp to exchange data. It can be used to implement efficient inter-thread communication. Optimize data layout for shared memory.\n\n## Conclusion\n\nThis article has outlined potential PTX optimizations employed by DeepSeek. These optimizations highlight DeepSeek's impressive ability to leverage fundamental hardware optimization, enabling them to develop models that effectively compete with OpenAI. The difficulty of these low level optimizations cannot be overstated.\n\n## Next\n\nIn my next article, we will get into the details of how these optimizations happen in various stages of the architecture, from MHLA to Multi-token.\n\n## References\n\n[DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)\\\n[DeepSeek EP Github](https://github.com/deepseek-ai/DeepEP)\n\n",
    "supporting": [
      "DeepSeekPTX_files"
    ],
    "filters": [],
    "includes": {}
  }
}