{
  "hash": "871a5efd8ce348620ee61e7702b177b9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Transformers\"\nauthor: \"Rakesh Venkat\"\ndate: \"2025-03-01\"\ncategories: [LLM]\nimage: \"img/LLMarch.png\"\njupyter: python3\n---\n\n# Introduction\n\nIn my previous blog post, I introduced Deepseek LLM's innovative parallel thread execution (PTX) mechanism and how they use it for GPU optimization. Today, we'll cover another foundational topic - **Multihead Attention (MHA)** before diving into Deepseek's second groundbreaking innovation known as **Multihead Latent Attention (MHLA)**.\n\n<!-- truncate -->\n\n## Multihead Latent Attention Roadmap\n\nLogical progression to understand MHLA is as shown in the image below.\n\n![MHLA](img/MHLA%20roadmap.png)\n\nThe attention mechanism was introduced to solve fundamental limitations in the **`Encoder-Decoder`** architecture. Let's examine why this was necessary.\n\n## Encoder-Decoder\n\nBefore attention mechanisms, sequence processing relied on RNN (**`Recurrent Neural Network`**) and LSTM (**`Long Short Term Memory`**) networks which had significant limitations:\n\n-   The encoder processes input token through LSTM/RNN cells, with the final hidden state (h3) passed to the decoder\n-   All information from previous hidden states (h0, h1, h2) is compressed into a single vector called the context vector\n-   This means that to generate outputs, the decoder only has access to this final hidden state\n-   As a result, contextual information is lost when processing longer sequences\n\n![Encoder-Decoder](img/Encoder-decoder.png)\n\n## Drawback of Encoder-Decoder\n\nThe traditional encoder-decoder suffered from several key limitations:\n\n**`Information Bottleneck`**: The entire input sequence had to be compressed into a single fixed-length context vector, regardless of the input sequence length.\\\n**`Long Range Dependencies`**: As sequence length increased, the model struggled to maintain relationships between positions.\\\n**`Vanishing Information`**: Information from the beginning of long sequences would \"fade\" by the time it reached the decoder.\n\nThese limitations were particularly problematic for machine translation tasks where sentences in different languages often have different structures and word orders.\n\n## Attention\n\nThe concept of **`Attention`** was introduced to solve the above challenges in a landmark paper **`Neural Machine Translation by Jointly Learning to Align and Translate`** by Bahdanau, Cho, and Bengio in 2014. It revolutionized the field of sequence processing by allowing neural networks to focus on specific parts of the input when generating outputs. You can find the paper [**here**](https://arxiv.org/pdf/1409.0473)\n\n::: info\nDecoder (s1) has now access to every hidden state and also the context of every hidden state in the encoder stage.\n:::\n\n![Encoder-Decoder-Attention](img/Encoder-decoder-attention.png)\n\nAnother way of imagining the Attention Mechanism is as below. The attention block in between has the context information of inputs and much more richer containing semantic meaning.\n\n::: info\nSee the attention weights giving the importance for each hidden state in the below image\n\nKey Improvement: The decoder (s1) now has access to every hidden state from the encoder stage, giving it context from the entire input sequence. The stronger colored bands in this visualization represent higher attention weights, showing which input tokens the model is focusing on when generating each output.\n:::\n\n![Attention](img/Attention.png)\n\nThe Bahdanau attention mechanism allowed decoder to \"look back\" at the entire sequence of encoder hidden states when generating each output token. Rather than relying solely on a fixed context vector, the decoder could dynamically focus on relevant parts of the input sequence.\n\n## Self Attention\n\nTo understand how everything fits together, let's revisit the transformer architecture:\n\n![LLM Architecture](img/LLMarch.png)\n\nAs a block schematic, self-attention will look something like this:\n\n![SelfAttention](img/SelfAttention-Calc.png)\n\nWhere:\n\n\\$ X : \\$ `Input embedding (Token Embedding + Positional Encoding)`\\\n\\$ W_Q : \\$ `Trainable Query Matrix`\\\n\\$ W_K : \\$ `Trainable Key Matrix`\\\n\\$ W_V : \\$ `Trainable Value Matrix`\\\n\\$ \\sqrt(d_k): \\$ `Square Root of keys dimensions`\n\n::: tip\nExample: See how the word \"Data\" interacts with surrounding words. Each word calculates attention scores with every other word in the sequence:\n\n![Self Attention](img/Self-attention-eg.png)\n:::\n\n::: info\nThe process works something like this:\n\n![Keysvalue](img/SelfAttention-KeysValue.png)\n\n`Attention Score will then be calculated as :`\\\n\\$ x_2 \\circ x_1 \\$\\\n\\$ x_2 \\circ x_2 \\$\\\n\\$ x_2 \\circ x_3 \\$\\\n\\$ x_2 \\circ x_4 \\$\n\n`Attention Weights will be :`\n\n\\$ Attention Weight = \\$ softmax $([\\alpha_{21},\\alpha_{22},\\alpha_{23},\\alpha_{24}])$ = $([w_{21},w_{22},w_{23},w_{24}])$\n\n`Finally Context Vector for \"Data\" will then be :`\n\n\\$ Context Vector\\_{Data} = \\$ \\$ (w\\_{21} \\circ v_1) + \\$ \\$ (w\\_{22} \\circ v_2) + \\$ \\$ (w\\_{23} \\circ v_3) + \\$ \\$ (w\\_{24} \\circ v_4) \\$\n\nWhere: $v_1 ,v_1, v_1, v_1$ are Value Matrix\n:::\n\n## Self Attention - Dimensions\n\n::: tip\nNote: Understanding tensors (multi-dimensional arrays) and matrix multiplication is essential here. With practice, these operations become intuitive.\n:::\n\nHere's how input text is processed through the self-attention blocks, including sample matrix dimensions:\n\n::: note\n-   The numbers in brackets represent dimensions (e.g., $W_Q$ has dimensions $[10, 5]$)\n-   This example processes 6 words (i.e. 6 tokens)\n-   The context vector output feeds into the logits layer to calculate probabilities for the next word\n-   While we show just one transformer block here, modern architectures stack multiple blocks\n:::\n\n![Attention-Matrix](img/attention-matrix.png)\n\n## Causal Attention\n\nCausal attention which is also known as `Masked Attention` is a variant used in language models that ensures tokens can only attend to themselves and previous tokens in the sequence. This maintains the autoregressive property needed for text generation, where each token is predicted based only on previously observed tokens.\n\n::: info\nAutoregression: The output for each word goes back as an input to predict the next word. ![AutoRegression](img/AutoregressiveModel.png)\n:::\n\nLet's take a simpler example: in the sentence `THE DATA IS HUGE`, when predicting the word `IS`, causal attention only needs to calculate attention scores for `THE` and `DATA` (previous and current tokens). This significantly reduces computation compared to full self attention, which would unnecessarily calculate scores for future tokens like `HUGE`.\n\n## Efficiency Advantage\n\nOne significant benefit of causal attention is computational efficiency. Since each token only needs to calculate attention weights for itself and preceding tokens but not future tokens, the number of calculations decreases substantially:\n\n-   For the first token: only 1 attention weight calculation\n-   For the second token: 2 attention weight calculations\n-   For the third token: 3 attention weight calculations\n-   And so on...\n\nIn causal attention, we apply a mask to the attention scores matrix that sets all future position scores to negative infinity before the softmax operation\n\n![CausalAttention](img/CausalAttention.png)\n\n## Multihead Attention\n\nBut why would you need Multihead attention? Have a look at this sentence - `The programmer compiled the code, but it still had bugs in it`.\n\nIn this sentence, we have two instances of \"it\". First \"it\" refers to \"complation of the code\" and second \"it\" refers \"bug in the code\". If we get the token ids then it will look something like this.\n\nBoth instances of 'it' have identical token IDs, but they refer to different concepts in the sentence. This shows why we need multiple perspectives of the input sequence, as a single attention mechanism might not capture these different contextual meanings.\n\n\\[10\\] \\[23\\] \\[3\\] \\[34\\] \\[50\\] \\[89\\] \\[14\\] **`[77]`** \\[69\\] \\[8\\] \\[15\\] \\[9\\] **`[77]`** \\[.\\]\n\nThat means the context is completly lost since it will have only one perspective. What if we somehow capture different perspective of a given input sequence?\n\nInstead of performing a single attention operation, multihead attention performs multiple attention in parallel. Each \"head\" learns different relationship or patterns or perspective:\n\n## Method\n\nTake a look at this image. Here the input dimension is split into two heads which captures two perspective of a sentence.\n\n![MultiHead](img/Multihead-atten.png)\n\n-   Split the embedding dimension into multiple heads\n-   Each head performs its own Query, Key, Value projections\n-   Calculate attention independently in each head\n-   Concatenate results and project back to original dimension\n\nThis allows the model to jointly attend to information from different representation subspaces, capturing different aspects of the input sequence.\n\nMathematically :-\n\n$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) \\circ W^O$\n\nWhere each head is:\\\n$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$\n\n![Matrix](img/selfAttenMatrix.png)\n\n## Code\n\nLet us see some code we discussed so far.\n\nAssume that the input \\$ x \\$ is the input embedding which is tokenized and positional encoding is applied\n\nOutput of \\$ x = \\$ \\[1, 3, 6\\] which is read as follows: *`3 rows , 6 columns and batch size is 1`*.\n\n``` python\n\n# decode d_out and number of heads\n# head_dimn = d_out/n_heads\n\nd_out = 6\nnum_heads = 2\nhead_dimn = int(d_out/num_heads)\nprint(\"output dimn is:\", d_out)\nprint(\"number of heads is:\", num_heads)\nprint(\"head dimn is:\", head_dimn)\n\nx = torch.tensor([[[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n                    [0.7, 0.8, 0.9, 0.10, 0.11, 0.12], \n                    [0.13, 0.14, 0.15, 0.16, 0.17, 0.18]]])\n\nbatch, tokenid, d_in = x.shape\n```\n\nTrainable matrix \\$ W_q, W_k, W_v \\$\n\n``` python\n\nW_Query = torch.nn.Linear(d_in, d_out, bias=False)\nW_Key = torch.nn.Linear(d_in, d_out, bias=False)\nW_Value = torch.nn.Linear(d_in, d_out, bias=False)\n\n# get the key , query and values matrix \nkeys = W_Key(x)\nquery = W_Query(x)\nvalues = W_Value(x)\n\n# 3 rows and 6 columns having 1 batch\nprint(query.shape)\nprint(keys.shape)\nprint(values.shape)\n\n\n# convert [batch, tokens, d_out] to [batch, token, num_heads, head_dimn]\n# 3 Dimensional [1,3,6] -> 4 Dimensional [1,3,2,3] i.e. 1 batch of 2x3 having 3 such sets\nkeys = keys.view(batch, tokenid, num_heads, head_dimn)\nquery = query.view(batch, tokenid, num_heads, head_dimn)\nvalues = values.view(batch, tokenid, num_heads, head_dimn)\n\nprint(keys.shape, query.shape, values.shape)\nkeys\n\n\natt_score = query @ keys.transpose(2,3) # this results in Q1 x K1(transpose) & Q2 x K2(transpose)\nprint(att_score.shape)\n# this is attention score of head1 and head 2 with tokenid in columns and tokenids in rows i.e. tokenids x tokenids\natt_score \n\n\n# To get attention weights we need to do the following\n# scaling by sqrt(key dimns) + softmax + causal attention + dropouts\n\n# first apply mask on the upper triangle of the matrix on tokenids which is 3x3\nmask = torch.triu(torch.ones(3,3), diagonal=1).bool()\nmask\n\n# we need to now change the mask to -inf so when we apply softmax, it will be turn to zeros\natt_score.masked_fill_( mask, float('-inf'))\n\n\nsqrt_d = keys.shape[-1]**0.5\nprint(\"square root of keys is : \", sqrt_d)\n\n# apply softmax on the last dimension which is length of the tokenids [batch, num_heads, tokenids, tokenids]\nattn_weights = torch.softmax(att_score/sqrt_d, dim=-1)\nattn_weights\n\n# context vector is attn_weights * value matrix\nvalues.shape, values\n\ncontext_vector = (attn_weights @ values)\ncontext_vector.shape, context_vector\n\n# if you want projection (optional) \ncontext_vec = torch.nn.Module.out_proj \n```\n\n## Conclusion\n\nNow that we've established a solid understanding of conventional attention mechanisms, in the next post we'll explore Deepseek's innovative **`Multihead Latent Attention (MHLA)`**. This technique represents a significant advancement that improves both computational efficiency and model performance by operating in a more compact latent space. MHLA reduces computational complexity while maintaining or even enhancing the model's ability to capture relationships between tokens, particularly for long sequences. Stay tuned to learn how this optimization technique can be applied to your own language models!\n\n",
    "supporting": [
      "transformers_files"
    ],
    "filters": [],
    "includes": {}
  }
}